
@online{andrychowiczLearningLearnGradient2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
  date = {2016-11-30},
  url = {http://arxiv.org/abs/1606.04474},
  urldate = {2021-06-04},
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archiveprefix = {arXiv},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  file = {/Users/eugene/Zotero/storage/DCNUQB95/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  options = {useprefix=true},
  primaryclass = {cs}
}

@article{kadirkamanathanFunctionEstimationApproach1993,
  title = {A {{Function Estimation Approach}} to {{Sequential Learning}} with {{Neural Networks}}},
  author = {Kadirkamanathan, Visakan and Niranjan, Mahesan},
  date = {1993-11-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {5},
  pages = {954--975},
  doi = {10.1162/neco.1993.5.6.954},
  abstract = {In this paper, we investigate the problem of optimal sequential learning, viewed as a problem of estimating an underlying function sequentially rather than estimating a set of parameters of the neural network. First, we arrive at a suboptimal solution to the sequential estimate that can be mapped by a growing gaussian radial basis function (GaRBF) network. This network adds hidden units for each observation. The function space approach in which the estimates are represented as vectors in a function space is used in developing a growth criterion to limit its growth. A simplification of the criterion leads to two joint criteria on the distance of the present pattern and the existing unit centers in the input space and on the approximation error of the network for the given observation to be satisfied together. This network is similar to the resource allocating network (RAN) (Platt 1991a) and hence RAN can be interpreted from a function space approach to sequential learning. Second, we present an enhancement to the RAN. The RAN either allocates a new unit based on the novelty of an observation or adapts the network parameters by the LMS algorithm. The function space interpretation of the RAN lends itself to an enhancement of the RAN in which the extended Kalman filter (EKF) algorithm is used in place of the LMS algorithm. The performance of the RAN and the enhanced network are compared in the experimental tasks of function approximation and time-series prediction demonstrating the superior performance of the enhanced network with fewer number of hidden units. The approach adopted here has led us toward the minimal network required for a sequential learning problem.},
  file = {/Users/eugene/Zotero/storage/WMQ55GIT/Kadirkamanathan and Niranjan - 1993 - A Function Estimation Approach to Sequential Learn.pdf}
}

@online{LearningSynapticLearning,
  title = {Learning a Synaptic Learning Rule | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/Learning-a-synaptic-learning-rule-Bengio-Bengio/d130325c41947a41a55a4431e9e8e15be89da8ea},
  urldate = {2021-06-04}
}

@online{liLearningOptimizeNeural2017,
  title = {Learning to {{Optimize Neural Nets}}},
  author = {Li, Ke and Malik, Jitendra},
  date = {2017-11-30},
  url = {http://arxiv.org/abs/1703.00441},
  urldate = {2021-06-04},
  abstract = {Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100.},
  archiveprefix = {arXiv},
  eprint = {1703.00441},
  eprinttype = {arxiv},
  file = {/Users/eugene/Zotero/storage/FHK3J9ZH/Li and Malik - 2017 - Learning to Optimize Neural Nets.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@online{maclaurinGradientbasedHyperparameterOptimization2015,
  title = {Gradient-Based {{Hyperparameter Optimization}} through {{Reversible Learning}}},
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
  date = {2015-04-02},
  url = {http://arxiv.org/abs/1502.03492},
  urldate = {2021-06-04},
  abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
  archiveprefix = {arXiv},
  eprint = {1502.03492},
  eprinttype = {arxiv},
  file = {/Users/eugene/Zotero/storage/5YHHG882/Maclaurin et al. - 2015 - Gradient-based Hyperparameter Optimization through.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@report{mahesanniranjanCOMP6247202021,
  title = {{{COMP6247}}(2020/21): {{Reinforcement}} and {{Online Learning}} - {{Final Assignment}}},
  author = {{Mahesan Niranjan} and {Christine Evers}},
  institution = {{School of Electronics and Computer Science, University of Southampton}},
  file = {/Users/eugene/dev/COMP6247-Labs/lab_4/COMP6247(202021) Reinforcement and Online Learni.pdf}
}

@online{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013-12-19},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2021-06-03},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  file = {/Users/eugene/Zotero/storage/BPTNEB44/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@report{mooreEfficientMemorybasedLearning1990,
  title = {Efficient {{Memory}}-Based {{Learning}} for {{Robot Control}}},
  author = {Moore, Andrew William},
  date = {1990},
  abstract = {This dissertation is about the application of machine learning to robot control. A system which has no initial model of the robot/world dynamics should be able to construct such a model using data received through its sensors--an approach which is formalized here as the \$AB (State-Action-Behaviour) control cycle. A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered. The experiences are stored in a manner which permits fast recall of the closest previous experience to any new situation, thus permitting very quick predictions of the effects of proposed actions and, given a goal behaviour, permitting fast generation of a candidate action. The learning can take place in high-dimensional non-linear control spaces with real-valued ranges of variables. Furthermore, the method avoids a number of shortcomings of earlier learning methods in which the controller can become trapped in inadequate performance which does not improve. Also considered is how the system is made resistant to noisy inputs and how it adapts to environmental changes. A well founded mechanism for choosing actions is introduced which solves the experiment/perform dilemma for this domain with adequate computational efficiency, and with fast convergence to the goal behaviour. The dissertation explefins in detail how the \$AB control cycle can be integrated into both low and high complexity tasks. The methods and algorithms are evaluated with numerous experiments using both real and simulated robot domefins. The final experiment also illustrates how a compound learning task can be structured into a hierarchy of simple learning tasks.},
  file = {/Users/eugene/Zotero/storage/8GET7XJX/Moore - 1990 - Efficient Memory-based Learning for Robot Control.pdf}
}

@article{parkUniversalApproximationUsing1991,
  title = {Universal {{Approximation Using Radial}}-{{Basis}}-{{Function Networks}}},
  author = {Park, J. and Sandberg, I. W.},
  date = {1991-06-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {3},
  pages = {246--257},
  issn = {0899-7667},
  doi = {10.1162/neco.1991.3.2.246},
  url = {https://doi.org/10.1162/neco.1991.3.2.246},
  urldate = {2021-06-02},
  abstract = {There have been several recent studies concerning feedforward networks and the problem of approximating arbitrary functionals of a finite number of real variables. Some of these studies deal with cases in which the hidden-layer nonlinearity is not a sigmoid. This was motivated by successful applications of feedforward networks with nonsigmoidal hidden-layer units.This paper reports on a related study of radial-basis-function (RBF) networks, and it is proved that RBF networks having one hidden layer are capable of universal approximation. Here the emphasis is on the case of typical RBF networks, and the results show that a certain class of RBF networks with the same smoothing factor in each kernel node is broad enough for universal approximation.},
  file = {/Users/eugene/Zotero/storage/WR3PZBE6/Park and Sandberg - 1991 - Universal Approximation Using Radial-Basis-Functio.pdf},
  number = {2}
}

@article{plattResourceallocatingNetworkFunction1991,
  title = {A Resource-Allocating Network for Function Interpolation},
  author = {Platt, John},
  date = {1991},
  journaltitle = {Neural Computation},
  abstract = {We have created a network that allocates a new computational unit whenever an unusual pattern is presented to the network. This network forms compact representations, yet learns easily and rapidly. The network can be used at any time in the learning process and the learning patterns do not have to be repeated. The units in this network respond to only a local region of the space of input values. The network learns by allocating new units and adjusting the parameters of existing units. If the network performs poorly on a presented pattern, then a new unit is allocated which corrects the response to the presented pattern. If the network performs well on a presented pattern, then the network parameters are updated using standard LMS gradient descent. We have obtained good results with our resource-allocating network (RAN). For predicting the Mackey Glass chaotic time series, our network learns much faster than do those using backpropagation and uses a comparable number of synapses. 1},
  file = {/Users/eugene/Zotero/storage/QJVU92UE/Platt - 1991 - A resource-allocating network for function interpo.pdf}
}

@article{rahimiRandomFeaturesLargeScale,
  title = {Random {{Features}} for {{Large}}-{{Scale Kernel Machines}}},
  author = {Rahimi, Ali and Recht, Ben},
  pages = {8},
  abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
  file = {/Users/eugene/Zotero/storage/2B3BEAKL/Rahimi and Recht - Random Features for Large-Scale Kernel Machines.pdf},
  langid = {english}
}

@software{teohEugeneteohCOMP6247ReinforcementOnlineLearning2021,
  title = {Eugeneteoh/{{COMP6247}}-{{Reinforcement}}-{{Online}}-{{Learning}}},
  author = {Teoh, Eugene},
  date = {2021-05-07T18:11:36Z},
  origdate = {2021-02-15T07:29:23Z},
  url = {https://github.com/eugeneteoh/COMP6247-Reinforcement-Online-Learning},
  urldate = {2021-05-27},
  abstract = {University of Southampton COMP6247 Reinforcement \& Online Learning 2020-2021}
}

@online{UCIMachineLearning,
  title = {{{UCI Machine Learning Repository}}: {{Airfoil Self}}-{{Noise Data Set}}},
  url = {https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise},
  urldate = {2021-05-27}
}

@article{wuUsingRadialBasis2012,
  title = {Using {{Radial Basis Function Networks}} for {{Function Approximation}} and {{Classification}}},
  author = {Wu, Yue and Wang, Hui and Zhang, Biaobiao and Du, K.-L.},
  date = {2012-03-06},
  journaltitle = {ISRN Applied Mathematics},
  volume = {2012},
  pages = {e324194},
  publisher = {{Hindawi}},
  doi = {10.5402/2012/324194},
  url = {https://www.hindawi.com/journals/isrn/2012/324194/},
  urldate = {2021-06-02},
  abstract = {The radial basis function (RBF) network has its foundation in the conventional approximation theory. It has the capability of universal approximation. The RBF network is a popular alternative to the well-known multilayer perceptron (MLP), since it has a simpler structure and a much faster training process. In this paper, we give a comprehensive survey on the RBF network and its learning. Many aspects associated with the RBF network, such as network structure, universal approimation capability, radial basis functions, RBF network learning, structure optimization, normalized RBF networks, application to dynamic system modeling, and nonlinear complex-valued signal processing, are described. We also compare the features and capability of the two models.},
  file = {/Users/eugene/Zotero/storage/IBVYPFM2/Wu et al. - 2012 - Using Radial Basis Function Networks for Function .pdf},
  langid = {english}
}


